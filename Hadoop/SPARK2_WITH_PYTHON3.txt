SPARK2 WITH PYTHON3
************************

RDD:


Basic Transformations and Actions - 01 - map, flatMap, reduce and more
***********************************************************************

1.Creating an RDD from a collection (pyspark shell where sparkContext is imported as sc)

l=range(1,100)
rdd=sc.parallelize(l)

2. Taking count from an rdd

rdd.count()

3. Applying a filter on rdd

rdd.filter(function)

example:
l=range(1,100)
lrdd=sc.parallelize(l)
lrdd.filter(lambda x : x%2==0) # this will return rdd which is a factor of 2

4.Applying a map on rdd

l=range(1,10)
lrdd=sc.parallelize(l)
lrdd.map(lambda x : (x,x**2)) # this will return an rdd of tuples with 1 to 9 numbers and their square as pair

5.Applying flatMap on rdd 
 l = [['a','b','c'],['d','e','f']]
 lrdd=sc.parallelize(l)
 lrdd.flatMap(lambda x : x) # this will unpack the inner lists and output will be a single list.
 
6.map() v/s flatMap()

A map is a transformation operation in Apache Spark. It applies to each element of RDD and it returns the result as new RDD. 
In the Map, operation developer can define his own custom business logic. The same logic will be applied to all the elements of RDD.
Spark Map function takes one element as input process it according to custom code (specified by the developer) and returns one element at a time. 
Map transforms an RDD of length N into another RDD of length N. The input and output RDDs will typically have the same number of records.Learn more on 

A flatMap is a transformation operation. It applies to each element of RDD and it returns the result as new RDD. It is similar to Map, 
but FlatMap allows returning 0, 1 or more elements from map function. 
In the FlatMap operation, a developer can define his own custom business logic. The same logic will be applied to all the elements of the RDD.
A FlatMap function takes one element as input process it according to custom code (specified by the developer) and returns 0 or more element at a time. 
flatMap() transforms an RDD of length N into another RDD of length M.

7.reduce()

This is an action. It converts rdd to a single value by performing some aggregation

l=range(1,10)
lrdd=sc.parallelize(l)
lrdd.reduce(lambda x,y : x+y) # this will give the sum of all numbers of rdd

8.Converting a text file on HDFS into an RDD
example:

rdd=sc.textFile("/user/pathirippilly/sample_data_mr/wordCount.txt",5) # here first argument is manadatory which is nothing but path of the file
and second argument is optional  which is number of partitions to be created


8.reduceByKey()

This is an action. It takes key value pairs as input and apply aggregation over a key

example:
Below is the word count program code where 

a.First file is converted in to rdd with specified partitions using sc.textFile()
b.Second words of each lines are splitted using flatMap()
c.Key value pairs of each word and '1' are formed using map()
d. And count of each words in the file are calculated using reduceByKey()

rdd=sc.textFile("/user/pathirippilly/sample_data_mr/wordCount.txt",5) #a
mappedrdd=rdd.flatMap(lambda x :x.split(" ")).map(lambda x :(x,1)) #b and c
shuffledRDD=mappedrdd.reduceByKey(lambda x,y : x+y) #d

9.groupByKey:

When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. 
Note: If you are grouping in order to perform an aggregation (such as a sum or average)
over each key, using reduceByKey or aggregateByKey will yield much better performance. 
Note: By default, the level of parallelism in the output depends on the number 
of partitions of the parent RDD. You can pass an optional numPartitions argument 
to set a different number of tasks.

example:

problem:
Find total  revenue per order number from the orderitems data file

orderitem file has following columns  seperated by comma as delimitter.

item_no,order_item_order_id,product_id,qty_sold,order_item_subtotatl,price_per_qty

Code:
orderitems=sc.textFile("/public/retail_db/order_items") # OrderItems file has been read here 
order_rev_pair=orderitems.map(lambda order : (int(order.split(",")[1]),float(order.split(",")[4]))) # Creating pairs of order_id and order_item_subtotal
revenueperorder=order_rev_pair.groupByKey().map(lambda x : (x[0],round(sum(x[1]),1))) # grouping and performing sum of order_item_subtotal per order_id

alternative:
The above same problem can be solved by reduceByKey in a more efficient way

code:

order_rev_pair.reduceByKey(lambda x,y : round(x+y,2))

10.SortByKey

It sorts based on Key of KeyValue Pair

From above example, if we take revenueperorder and if we apply

revenueperorder.sortByKey() # this will sort the dataset in ascending order of order number
revenueperorder.sortByKey(False) # this will sort dataset in descending order of order number
revenueperorder.map(lambda x: (x[1],x)).sortByKey(False).map(lambda x :x[1]).take(10) # this will give you orders with  top ten revenue

11.Dealing with partitions 

if you are reading from a file to rdd in HDFS , by default it will create number of partitions equals number of blocks it has to read.
Say Default block size of HDFS (2.x)is 128mb, then if a file less than this size  will be read to rdd by spark as 1 partition since only one block is used.
And if it is 256mb file, then 2 partitions and so on.

To know the current number of partitions of an rdd,

orderitems=sc.textFile("/public/retail_db/order_items")
orderitems.getNumPartitions()

To know , number of records in each partition, you can use

orderitems.glom().map(lambda x : len(x))

Now If I save it directly in to HDFS:
orderitems.saveAsTextFile("/user/pathirippilly/spark_jobs/output_data_set/orderitesmpartitioned")

we can see the how many number of partitions are there, that many number of files will be created.

To alter the default partitions created, we can do it in multiple ways

method 1:
While reading a file to rdd  using sparkContext, you can pass the second argument as numPartitions as below 
orderitems=sc.textFile("/public/retail_db/order_items",5) # Here 5 partitions will be created irrespective of the size of the file 

method 2 :
If you want to decrease the number of partitions , we can use coalesce.

orderitems=sc.textFile("/public/retail_db/order_items")
orderitems.coalesce(1) # this will reduce the number of partitions to 1

note:
It avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions,
only moving the data off the extra nodes, onto the nodes that we kept.

method 3:
This is using repartition.This is slower and expensive than coalesce(But you can increase or decrease partitions here)
orderitems=sc.textFile("/public/retail_db/order_items")
orderitems.repartition(5)# Here we are repartitioning to 5

Note:
Keep in mind that repartitioning your data is a fairly expensive operation.  


12.groupByKey vs reduceByKey vs aggregateByKey:  why reduceByKey and aggregateByKey is preferred in spark2

The main difference is the number of records that are tansferring from stage 0  to 1 during shuffling face:

Bykey operations always results in shuffling , since grouping of records between multiple partitions  are needed
In such case, result of shuffling is a new stage with a modified number of partitions


Lets take the  code for calculating order revenue :

orderitems=sc.textFile("/public/retail_db/order_items") # OrderItems file has been read here 
order_rev_pair=orderitems.map(lambda order : (int(order.split(",")[1]),float(order.split(",")[4]))) # Creating pairs of order_id and order_item_subtotal
revenueperorder=order_rev_pair.groupByKey().map(lambda x : (x[0],round(sum(x[1]),1))) # grouping and performing sum of order_item_subtotal per order_id

whats happening in shuffle phase for groupByKey:

Here we are applying  groupByKey and on order_rev_pair rdd, which is nothing but an rdd with key value pairs.
Key is order id which is an integer.

So when we apply groupByKey on a rdd which needs to have key value pairs, first it needs to be shuffled based on key.

Shuffling  always heppens with hash mod on no.of partitions

HashmodKey:

hash mod Key is nothing but , (hash value of the key) modulo (no.of partitions of the rdd)

in python hash value of any integer is the integer itself,for string its different.
For finding hash value , we can use
hash(key) # where key can be string or integer

Any way in our example , key is an integer.so hash(key) is also the same.
Lets say we have below key value pairs in one partition  and similarly  we have 3 partitions as a whole.

(5,299.95) => applying hash mod no.partitions , 5 mod 3 => 2
(4,299.95) => applying hash mod no.partitions , 4 mod 3 => 1
(2,129.99) => applying hash mod no.partitions , 2 mod 3 => 1
(5,129.99) => applying hash mod no.partitions , 5 mod 3 => 2

we can see result of mod operation is two distinct values 2,1. 
All values with hash mod value 2 will come under one bucket(partition)
and all with 1 will come under another bucket. 

Like this, for 3 existing partitions, we have 12 records.
And possible hash mod values are only three - 0,1,2.(0 can also come if we have 3 as key - 3 mod 3 is zero)
So possible number of partitions after shuffling are three.

So all values with same hash mode value will come under same bucket.
They will be grouped and sorted automatically.In above case it will be as below

bucket 1:
(2,129.99)
(4,299.95)
bucket 2:
(5,299.95)
5,129.99)

(Bucket 0 also can have value which I am not mentioning here)

Now this is stage 1 where groupByKey is happening .

So all records  with same order_id will be grouped together
(5,[299.95,129.99])
(2,129.99)
(4,299.95)

If you see eventhough its grouped , no.of records transferred from stage 0 to stage 1 is same
And Now we need an extra map function here to perform aggregation


whats happening in shuffle phase for reduceByKey:

If we take the same scenario as above , while performing hash mod operation itself on stage 0,
reduceByKey will be performed for each partition and bucket 1 and bucket 2 of stage 1 will look like this
 
 bucket 1:
(2,129.99)
(4,299.95)
bucket 2:
(5,429.94)

like this for every partition in stage 0 itself , intermediate aggregation will be done by reduceByKey
And in stage 1, final  level of aggregation will be performed by reduceByKey.

So a combiner implemenation is there here due to which records transferred between stages are less.


This difference will be much high in case of large data sets so that if we use  reduceByKey , 'shuffle read' size will be 
less

Now aggregateByKey is as same as reduceByKey but only difference is ,we will be providing the intermediate combiner logic as a function.

If we take the above scenario itself, along with total revenue , if we need to calculate the total number of items per order,
we need to , we have to perform to aggregations together, so intermediate logic will change


order_rev_pair.aggregateByKey((0.0,0),lambda x,y: (x[0]+y,x[1]+1),lambda x,y:x[0]+y[0],(x[1]+y[1]))

Here we first tuple is initialization of desired output, second argument is the combiner logic which will be executed on stage 0.
And third argument is the final aggregation logic which will be executed on stage1

13.Filter , Joins and sortByKey()

Lets see how filter works:

filter() is similar to where clause of an sql. Say we need to filter orders with 'CLOSED' and 'COMPLETE' status 
from a data set called orders with fileds order_id,order_tmst,order_customer_id,order_status. The code is as given below:

orders=sc.textFile("/public/retail_db/orders")
orders.filter(lambda x : x.split(",")[3] in ['CLOSED','COMPLETE']) # this will filter out All CLOSED and COMPLETE orders

Lets see how join works:

As in sql we have join , leftOuterJoin, rightOuterJoin, and fullOuterJoin.

So to demonstrate that , we can take below data sets 

orders with columns order_id,order_tmst,order_customer_id,order_status
orderitems with columns order_item,order_item_order_id,product_id,order_qty,order_item_subtotal,price_per_qty

#reading files in to rdd:

orders=sc.textFile("/public/retail_db/orders")
orderitems=sc.textFile("/public/retail_db/order_items")

#Checking the count of orders in each data set:
orders.count() # this will give  68883
orderitems.map(lambda x : x.split(",")[1]).distinct().count() # this will give 57431(we are first filtering out orders alone and applying distinct() over it because, 
#we have multiple items for a single order in this table)
# above difference shows that we have 68883-57431=11452 orders with no order items 

#Now lets take order_id and status alone from 'orders'.Also map order_item_order_id as key and each record as value (tuple) from 'orderitems' dataset
closed_complete_orders=orders.filter(lambda x : x.split(",")[3] in ['CLOSED','COMPLETE']).map(lambda x : (x.split(",")[0], x.split(",")[3])) #closed or complete orders from 'orders'
orderitems_mapped=orderitems.map(lambda x : (x.split(",")[1],x)) # Each record will be a tuple with order_item_order_id as key and entire line of record as value

closed_complete_orders.join(orderitems_mapped) # This will give closed or complete orders with orderitems
closed_complete_orders.leftOuterJoin(orderitems_mapped).filter(lambda x : x[1][1]==None).count() # this will give closed or complete orders with no orderitems in orderitem data set
#Same can be achieved with rightOuterJoin by just swapping the left and right data sets as below
orderitems_mapped.rightOuterJoin(closed_complete_orders).filter(lambda x : x[1][0]==None)
#Now if you do a fullOuterJoin on both , you will get all matched records from both table as well as un matched records with None as value


# Now lets see one of its application with sortByKey()


scenario 1:
we need to find total revenue per order_tmst joining data sets orders and order_items

lets take the two data sets 
orders with columns order_id,order_tmst,order_customer_id,order_status
orderitems with columns order_item,order_item_order_id,product_id,order_qty,order_item_subtotal,price_per_qty



#reading files in to rdd:

orders=sc.textFile("/public/retail_db/orders")
orderitems=sc.textFile("/public/retail_db/order_items")

#filter closed and complete orders alone from orders and extract only order_id and order_tmst from 'orders':

orders_closed_complete=orders.filter(lambda x : x.split(",")[3] in ['CLOSED','COMPLETE']).map(lambda x : (int(x.split(",")[0]),x.split(",")[1]))

#extract order_item_order_id,order_item_subtotal from 'orderitems':

orderitems_extract=orderitems.map(lambda x : (int(x.split(",")[1]),float(x.split(",")[4])))

#Join both 'orders_closed_complete' and 'orderitems_extract' on order_id and extract  order_tmst and order_item_subtotal  column values alone

orders_with_subtotal=orders_closed_complete.join(orderitems_extract).map(lambda x : x[1])

#Now apply reduceByKey  to get total revenue per order date:

from operator import add
order_rev=orders_with_subtotal.reduceByKey(add).map(lambda x:(x[0],round(x[1],2)))

#Now Sort the output based on date in ascending order:

order_rev=order_rev.sortByKey()

#Now Sort the output based on date in descending order:

order_rev=order_rev.sortByKey(False)


#Now Saving the output to a file.Here we need to convert tuple into a single string before writing into a file.
order_rev_string=order_rev.map(lambda x : f"{x[0]},{x[1]}")
order_rev_string.saveAsTextFile("out_put_path in hdfs") # output path should not exist. Also output splits are depends up on no.of partitions of rdd

14.map() vs mapPartitions():

If you use map() over an rdd , the function called  inside it will run for every record .It means if you have 10M records , function also will be executed 10M times.
This is expensive especially when you are dealing with scenarios involving database connections and querying data from data base.
Lets say inside map function, we have a function defined where we are connecting to a database and querying from it. Lets say our RDD is having 10M records.
Now this function will execute 10M times which means 10M database connections will be created . This  is very expensive.
In these kind of scenarios , we can use mapPartitions() instead of map().mapPartitions() will run the function called only once for each partitions.

example:

We can do a simple word count job for demonstrating this :

lets consider the below dataset:

/public/randomtextwriter/part-m-00000

This dataset is of 1 GB so that by default it will be read as 	rdd with 9 partitions by spark from HDFS since default block size
of HDFS is 128mb

wordcounttext = sc.textFile("/public/randomtextwriter/part-m-00000") # rdd with 9 partitions

Normal Wordcount logic using map is as below:

wordcounttext.flatMap(lambda x : x.split(" ")).map(lambda x : (x,1)).reduceByKey(lambda x,y : x+y)

Here rdd 'wordcounttext' is having 26421 records. flapMap will execute 26421 times.

But if we implement mapPartition here , the function defined inside it will execute only once for every partition.
And each partition will be read as a generator through which we can iterate and get the line of records.
So dealing with this generator can be done through core-python APIs

The above word count program can be written in two ways as below 

method1:
 wordcount=wordcounttext.mapPartitions(lambda x : map(lambda o:o.split(" "),x)).flatMap(lambda x : x).map(lambda x : (x,1)).reduceByKey(add)
 
method2:
here we define a function first which  will consume generator as an argument and iterate through it using itertools.chain.from_iterable().
itertools.chain.from_iterable()	functions same as flatMap in spark so that for every partition (every generator) picked by mapPartitions()
,it will convert the records in to independent words and words will be paired with '1'.And the output for each partition by this function will be
a map object which inturn is a generator. And this will be consumed by the lambda function inside mapPartitions()


def getWordTuples(i):
	import itertools as it
	wordTuples=map(lambda s : (s,1),it.chain.from_iterable(map(lambda s : s.split(" "),i)))
	return wordTuples
	
wordTuples = lines.mapPartitions(lambda i : getWordTuples(i)) # getWordTuples will execute for every partition
wordcount=wordTuples.reduceByKey(add)


	







pyspark --master yarn --conf spark.ui.port=21111 --conf spark.dynamicallocationenabled=false --num-executors 2 --executor-memory 512M






