Spark Architecture:

spark follows a Master/Slave architecture.There are 3 main components for spark

1. Driver Program : Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program which is
   nothing but the driver program 
2. Cluster Manager :  




All that you are going to do with Apache Spark is:

1. read some data from a source  
2. load it in to an object in memory 
3. Process it using Transformation and actions 
4. save it in a target 


Spark can store data in any of the below different objects

RDD : Resilient Distributed Dataset
DataFrames	
DataSets

From spark 2.x , it is recommended to use DataFrames and Datasets than RDD

Eventhough, DataFrames and Datasets will be converted to RDDs by spark while ruinning the program

RDDs are splitted into different partitions and distributed among different nodes internally by spark.




DataFrames
************

1. Launching pyspark shell:
pyspark --master yarn --conf spark.ui.port=12901 

2. Creating RDD/DataFrames

>> Creating RDD and converting in to DF

FILE TO RDD conversions:

1. A file stored in HDFS file system can be converted into an RDD using SparkContext itself.Since sparkContext can read the file directly from HDFS,
it will convert the contents  directly in to a spark RDD (Resilient Distributed Data Set)
in a spark CLI, sparkContext is imported as sc
Example: Reading from a text file
textRDD = sc.textFile("HDFS_path_to_text_file")

2. A file stored in local File system can not be read by sparkContext directly. So we need to read it using core python APIs as list and then need to convert 
it in to an RDD using sparkContext

example:

with open("local_path_to_file") as file:
 file_list=file.read().splitlines() #this will convert each line of the file in to an element of                                                            list.Here file_list have each line of the file as string
fileRDD = sc.parallelize(file_list) # This will convert the list in to an RDD where each element is of type string



RDD to DF conversions:
***************************

RDD is nothing but a distributed collection.By Default when you will read from a file to an RDD, each line  will be an element of type string.

DF (Data frame) is a structured representation of RDD. 

To convert an RDD of type tring to a DF,we need to either convert the type of RDD elements in to a tuple,list,dict or Row type

As an Example, lets say a file orders containing 4 columns of data ('order_id','order_date','customer_id','status')  in which each column is delimited by Commas.

And Let us assume, the file has been read using sparkContext in to an RDD (using one of the methods mentioned above) and RDD name is 'ordersRDD'

Now let us convert the RDD in to DF:

There are 4 ways:


RDD to DF using tuples:
***************************

#Here we are passing column names as a list

ordersTuple=ordersRDD.map(lambda o: (int(o.split(",")[0]),o.split(",")[1],int(o.split(",")[2]),o.split(",")[3])) 

ordersTuple.toDF(['order_id','order_date','customer_id','status'])


DD to DF using Row:
***************************

from pyspark.sql import Row;

method1:

#Here we are passing column names as a list

ordersRow=ordersRDD.map(lambda o: Row(int(o.split(",")[0]),o.split(",")[1],int(o.split(",")[2]),o.split(",")[3]))

ordersRow.toDF(['order_id','order_date','customer_id','status'])



method2:

#Here we are passing column names at the time of mapping itslef, a kind of similar to dict

ordersRow=ordersRDD.map(lambda o: Row(order_id=int(o.split(",")[0]),order_date=o.split(",")[1],customer_id=int(o.split(",")[2]),status=o.split(",")[3]))

ordersRow.toDF()


RDD to DF using List:
***************************


#Here we are passing column names as a list

ordersList=ordersRDD.map(lambda o: [int(o.split(",")[0]),o.split(",")[1],int(o.split(",")[2]),o.split(",")[3]])

ordersList.toDF(['order_id','order_date','customer_id','status'])


RDD to DF using dictionary (This is depricated and the similar method is using Row type. Even though still we can use it (verified in spark 2.3.1)):
************************************

#Here we are passing column names at the time of mapping itself

ordersDict=ordersRDD.map(lambda o: {'order_no':int(o.split(",")[0]),'order_date':o.split(",")[1],'customer_id':int(o.split(",")[2]),'status':o.split(",")[3]})

ordersDict.toDF()




